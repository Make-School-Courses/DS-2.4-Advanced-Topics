{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "- It is unsupervised learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the TF-IDF matrix for the following documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', 'app', 'belly', 'best', 'came', 'cat', 'chrome', 'climbing', 'eating', 'extension', 'face', 'feedback', 'google', 'impressed', 'incredible', 'key', 'kitten', 'kitty', 'little', 'map', 'merley', 'ninja', 'open', 'photo', 'play', 'promoter', 'restaurant', 'smiley', 'squooshy', 'tab', 'taken', 'translate', 've']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "documents = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 33)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  \"'nndsvda' in 1.1 (renaming of 0.26).\"), FutureWarning)\n",
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1091: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "n_topics = 2\n",
    "nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why we call it non-negative matrix factorization?\n",
    "\n",
    "- The BoW and TF-IDF matrix elements are all non-negative (zero or positive) \n",
    "\n",
    "- We want to build matrices W and H such that WH returns TF-IDF matrix\n",
    "\n",
    "- Such that all elements in W and H be non-negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  \"'nndsvda' in 1.1 (renaming of 0.26).\"), FutureWarning)\n",
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1091: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.        , 0.45217213],\n",
       "       [0.55735742, 0.        ],\n",
       "       [0.49414046, 0.        ],\n",
       "       [0.        , 0.74849032],\n",
       "       [0.        , 0.5964714 ],\n",
       "       [0.55735742, 0.        ],\n",
       "       [0.52368298, 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = nmf.fit_transform(tfidf)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 33)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = nmf.components_\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18654138, 0.26513516, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.21900044, 0.        , 0.        , 0.21900044,\n",
       "        0.18654138, 0.26513516, 0.59338024, 0.26513516, 0.26513516,\n",
       "        0.21900044, 0.        , 0.        , 0.        , 0.26513516,\n",
       "        0.        , 0.        , 0.18654138, 0.        , 0.        ,\n",
       "        0.21900044, 0.        , 0.18654138, 0.        , 0.18654138,\n",
       "        0.        , 0.26513516, 0.        ],\n",
       "       [0.        , 0.        , 0.18609847, 0.42271703, 0.        ,\n",
       "        0.53814516, 0.        , 0.32382804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.18609847, 0.        , 0.        , 0.        ,\n",
       "        0.18609847, 0.32382804, 0.        , 0.31829015, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.18609847, 0.        ,\n",
       "        0.31829015, 0.        , 0.31829015]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify we can reconstruct TF-IDF matrix from WH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# tfidf - np.dot(W, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the top 10 words of each topic from H matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "google feedback map app impressed incredible translate key extension chrome\n",
      "Topic #1:\n",
      "cat best climbing ninja ve photo taken belly merley kitten\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Modelling with NMF:\n",
      "Topic 0:\n",
      "google feedback map app impressed incredible translate key extension chrome\n",
      "Topic 1:\n",
      "cat best climbing ninja ve photo taken belly merley kitten\n",
      " ---- \n",
      "Topic Modelling with LDA\n",
      "Topic 0:\n",
      "google smiley translate restaurant tab promoter eating face feedback kitty\n",
      "Topic 1:\n",
      "cat best taken merley belly kitten squooshy ve ninja climbing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# no_features = 100\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 2\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1).fit(tfidf)\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "print('Topic Modelling with NMF:')\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print(' ---- ')\n",
    "print('Topic Modelling with LDA')\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.58778666 0.58778666 0.58778666 0.69314718 0.84729786\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.58778666 0.58778666 0.58778666 1.17360019 0.\n",
      "  1.09861229 0.         0.         0.        ]\n",
      " [0.         0.58778666 0.58778666 0.58778666 0.         0.\n",
      "  0.         1.09861229 1.09861229 1.09861229]\n",
      " [0.         0.58778666 0.58778666 0.58778666 0.69314718 0.84729786\n",
      "  0.         0.         0.         0.        ]]\n",
      "[[1, 2, 3, 5, 4], [1, 4, 2, 3, 6, 4], [7, 1, 2, 3, 8, 9], [2, 1, 3, 5, 4]]\n",
      "Word Index:\n",
      "{'this': 1, 'is': 2, 'the': 3, 'document': 4, 'first': 5, 'second': 6, 'and': 7, 'third': 8, 'one': 9}\n",
      "Word Counts:\n",
      "OrderedDict([('this', 4), ('is', 4), ('the', 4), ('first', 2), ('document', 4), ('second', 1), ('and', 1), ('third', 1), ('one', 1)])\n",
      "Document Count:\n",
      "4\n",
      "Words in Doc:\n",
      "defaultdict(<class 'int'>, {'the': 4, 'document': 3, 'is': 4, 'first': 2, 'this': 4, 'second': 1, 'one': 1, 'and': 1, 'third': 1})\n",
      "[[0 0 0 0 0 0 0 1 2 3 5 4]\n",
      " [0 0 0 0 0 0 1 4 2 3 6 4]\n",
      " [0 0 0 0 0 0 7 1 2 3 8 9]\n",
      " [0 0 0 0 0 0 0 2 1 3 5 4]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "documents = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(documents)\n",
    "mat_texts = tok.texts_to_matrix(documents, mode='tfidf')\n",
    "print(mat_texts)\n",
    "X = tok.texts_to_sequences(documents)\n",
    "print(X)\n",
    "print('Word Index:')\n",
    "print(tok.word_index)\n",
    "print('Word Counts:')\n",
    "print(tok.word_counts)\n",
    "print('Document Count:')\n",
    "print(tok.document_count)\n",
    "print('Words in Doc:')\n",
    "print(tok.word_docs)\n",
    "# Do zero padding\n",
    "print(sequence.pad_sequences(X, maxlen=12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Explore Embedding Layer in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]\n",
      " [5 1 2 3 6]]\n",
      "[[[-0.00455996 -0.01137421 -0.03638158]\n",
      "  [-0.02391403  0.00018699 -0.02607498]\n",
      "  [-0.00176594 -0.03788233 -0.04019349]\n",
      "  [ 0.04390157  0.03704363 -0.02197335]\n",
      "  [-0.03285757  0.03218425  0.00507914]]\n",
      "\n",
      " [[ 0.01627734  0.02665022  0.03782253]\n",
      "  [-0.02391403  0.00018699 -0.02607498]\n",
      "  [-0.00176594 -0.03788233 -0.04019349]\n",
      "  [ 0.04390157  0.03704363 -0.02197335]\n",
      "  [ 0.03163853  0.04659661 -0.018092  ]]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten\n",
    "import numpy as np\n",
    "\n",
    "# check this out:\n",
    "# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "model = Sequential()\n",
    "model.add(Embedding(7, 3, input_length=5))\n",
    "# model.add(Flatten())\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be\n",
    "# no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.array([[0, 1, 2, 3, 4], [5, 1, 2, 3, 6]])\n",
    "print(input_array)\n",
    "# model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array)\n",
    "# print(model(input_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Classification with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification + Keras:\n",
    "\n",
    "https://www.kaggle.com/psyhoo/spam-sms-neural-networks-in-keras\n",
    "\n",
    "num_max: The entire vocabulary we have in corpus -> vocab_size\n",
    "\n",
    "max_len: The maximum number of words per row (how many words a ham or spam email has) in corpus \n",
    "\n",
    "Goal for this Kaggle is exploring different Deep Learning models and its required text pre-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_to_category_GloVe(keyword_list):\n",
    "    dic = {}\n",
    "    with codecs.open('glove.840B.300d.txt', 'r') as f:\n",
    "    # with codecs.open('glove.6B.300d.txt', 'r', 'utf-8') as f:\n",
    "        for c, r in enumerate(f):\n",
    "            sr = r.split()\n",
    "            # if sr[0] in keyword_list + category_list:\n",
    "            if sr[0] in [i.encode() for i in keyword_list]:\n",
    "                # print(sr[0])\n",
    "                dic[sr[0]] = [float(i) for i in sr[1:]]\n",
    "                # print(c)\n",
    "                if len(dic) == len(keyword_list):\n",
    "                    break\n",
    "    category_list = pickle.load(open(\"category2.p\", \"rb\"))\n",
    "    category = {}\n",
    "    for i in keyword_list:\n",
    "        distance = []\n",
    "        for j in category_list:\n",
    "            distance.append([j, np.linalg.norm(np.array(dic[i])-np.array(category_list[j]))])\n",
    "        di = [s[0] for s in distance]\n",
    "        mi = [s[1] for s in distance]\n",
    "        idx = mi.index(min(mi))\n",
    "        category[i] = di[idx]\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keyword_to_category_GloVe([u'runner', u'pizza', u'physics', u'adidas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained word embeddings in a Keras model\n",
    "\n",
    "- https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to build Word2Vec using Tensorflow\n",
    "\n",
    "https://www.youtube.com/watch?v=64qSgA66P-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW is also a possible model for Word2Vec\n",
    "\n",
    "- https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA: Which sentences is what document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04624001 0.95375999]\n",
      " [0.06901926 0.93098074]\n",
      " [0.91056403 0.08943597]\n",
      " [0.0516827  0.9483173 ]\n",
      " [0.08450956 0.91549044]\n",
      " [0.85683624 0.14316376]\n",
      " [0.91108699 0.08891301]\n",
      " [0.92322557 0.07677443]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "\n",
    "# the dataset to predict on (first two samples were also in the training set so one can compare)\n",
    "documents = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]\n",
    "# Vectorize the training set using the model features as vocabulary\n",
    "tf_vectorizer = CountVectorizer()\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=2).fit(tf)\n",
    "\n",
    "# transform method returns a matrix with one line per document, columns being topics weight\n",
    "predict = lda.transform(tf)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62680148, 0.37319852]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf_vectorizer.transform(['cat is kitty'])\n",
    "lda.transform(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "- What is the dimension of m (`m.shape`)?\n",
    "\n",
    "- Can anyone come to the board and obtain the m?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
