{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# GLOVE_DIR = './glove.6B/'\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# data = pd.read_csv('../input/spam.csv',encoding='latin-1')\n",
    "data = pd.read_csv('./spam.csv',encoding='latin-1')\n",
    "\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "print(data.head())\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import metrics\n",
    "\n",
    "num_max = 1000\n",
    "le = LabelEncoder()\n",
    "tags = le.fit_transform(tags)\n",
    "tok = Tokenizer(num_words=num_max)\n",
    "tok.fit_on_texts(texts)\n",
    "# mat_texts = tok.texts_to_matrix(texts,mode='count')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(texts,tags, test_size = 0.3)\n",
    "# mat_texts_tr = tok.texts_to_matrix(x_train,mode='count')\n",
    "# mat_texts_tst = tok.texts_to_matrix(x_test,mode='count')\n",
    "\n",
    "max_len = MAX_SEQUENCE_LENGTH\n",
    "x_train = tok.texts_to_sequences(x_train)\n",
    "x_test = tok.texts_to_sequences(x_test)\n",
    "\n",
    "word_index = tok.word_index\n",
    "\n",
    "cnn_texts_mat = sequence.pad_sequences(x_train,maxlen=max_len)\n",
    "# max_len = 100\n",
    "cnn_texts_mat_tst = sequence.pad_sequences(x_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 1\n",
      "to 2\n",
      "you 3\n",
      "a 4\n",
      "the 5\n",
      "u 6\n",
      "and 7\n",
      "in 8\n",
      "is 9\n",
      "me 10\n",
      "my 11\n",
      "for 12\n",
      "your 13\n",
      "it 14\n",
      "of 15\n",
      "call 16\n",
      "have 17\n",
      "on 18\n",
      "2 19\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for word, index in word_index.items():\n",
    "    c += 1\n",
    "    if c <20:\n",
    "        print(word, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8920"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "of [-0.1529   -0.24279   0.89837   0.16996   0.53516   0.48784  -0.58826\n",
      " -0.17982  -1.3581    0.42541   0.15377   0.24215   0.13474   0.41193\n",
      "  0.67043  -0.56418   0.42985  -0.012183 -0.11677   0.31781   0.054177\n",
      " -0.054273  0.35516  -0.30241   0.31434  -0.33846   0.71715  -0.26855\n",
      " -0.15837  -0.47467   0.051581 -0.33252   0.15003  -0.1299   -0.54617\n",
      " -0.37843   0.64261   0.82187  -0.080006  0.078479 -0.96976  -0.57741\n",
      "  0.56491  -0.39873  -0.057099  0.19743   0.065706 -0.48092  -0.20125\n",
      " -0.40834   0.39456  -0.02642  -0.11838   1.012    -0.53171  -2.7474\n",
      " -0.042981 -0.74849   1.7574    0.59085   0.04885   0.78267   0.38497\n",
      "  0.42097   0.67882   0.10337   0.6328   -0.026595  0.58647  -0.44332\n",
      "  0.33057  -0.12022  -0.55645   0.073611  0.20915   0.43395  -0.012761\n",
      "  0.089874 -1.7991    0.084808  0.77112   0.63105  -0.90685   0.60326\n",
      " -1.7515    0.18596  -0.50687  -0.70203   0.66578  -0.81304   0.18712\n",
      " -0.018488 -0.26757   0.727    -0.59363  -0.34839  -0.56094  -0.591\n",
      "  1.0039    0.20664 ]\n",
      "to [-1.8970e-01  5.0024e-02  1.9084e-01 -4.9184e-02 -8.9737e-02  2.1006e-01\n",
      " -5.4952e-01  9.8377e-02 -2.0135e-01  3.4241e-01 -9.2677e-02  1.6100e-01\n",
      " -1.3268e-01 -2.8160e-01  1.8737e-01 -4.2959e-01  9.6039e-01  1.3972e-01\n",
      " -1.0781e+00  4.0518e-01  5.0539e-01 -5.5064e-01  4.8440e-01  3.8044e-01\n",
      " -2.9055e-03 -3.4942e-01 -9.9696e-02 -7.8368e-01  1.0363e+00 -2.3140e-01\n",
      " -4.7121e-01  5.7126e-01 -2.1454e-01  3.5958e-01 -4.8319e-01  1.0875e+00\n",
      "  2.8524e-01  1.2447e-01 -3.9248e-02 -7.6732e-02 -7.6343e-01 -3.2409e-01\n",
      " -5.7490e-01 -1.0893e+00 -4.1811e-01  4.5120e-01  1.2112e-01 -5.1367e-01\n",
      " -1.3349e-01 -1.1378e+00 -2.8768e-01  1.6774e-01  5.5804e-01  1.5387e+00\n",
      "  1.8859e-02 -2.9721e+00 -2.4216e-01 -9.2495e-01  2.1992e+00  2.8234e-01\n",
      " -3.4780e-01  5.1621e-01 -4.3387e-01  3.6852e-01  7.4573e-01  7.2102e-02\n",
      "  2.7931e-01  9.2569e-01 -5.0336e-02 -8.5856e-01 -1.3580e-01 -9.2551e-01\n",
      " -3.3991e-01 -1.0394e+00 -6.7203e-02 -2.1379e-01 -4.7690e-01  2.1377e-01\n",
      " -8.4008e-01  5.2536e-02  5.9298e-01  2.9604e-01 -6.7644e-01  1.3916e-01\n",
      " -1.5504e+00 -2.0765e-01  7.2220e-01  5.2056e-01 -7.6221e-02 -1.5194e-01\n",
      " -1.3134e-01  5.8617e-02 -3.1869e-01 -6.1419e-01 -6.2393e-01 -4.1548e-01\n",
      " -3.8175e-02 -3.9804e-01  4.7647e-01 -1.5983e-01]\n",
      "and [-0.071953  0.23127   0.023731 -0.50638   0.33923   0.1959   -0.32943\n",
      "  0.18364  -0.18057   0.28963   0.20448  -0.5496    0.27399   0.58327\n",
      "  0.20468  -0.49228   0.19974  -0.070237 -0.88049   0.29485   0.14071\n",
      " -0.1009    0.99449   0.36973   0.44554   0.28998  -0.1376   -0.56365\n",
      " -0.029365 -0.4122   -0.25269   0.63181  -0.44767   0.24363  -0.10813\n",
      "  0.25164   0.46967   0.3755   -0.23613  -0.14129  -0.44537  -0.65737\n",
      " -0.042421 -0.28636  -0.28811   0.063766  0.20281  -0.53542   0.41307\n",
      " -0.59722  -0.38614   0.19389  -0.17809   1.6618   -0.011819 -2.3737\n",
      "  0.058427 -0.2698    1.2823    0.81925  -0.22322   0.72932  -0.053211\n",
      "  0.43507   0.85011  -0.42935   0.92664   0.39051   1.0585   -0.24561\n",
      " -0.18265  -0.5328    0.059518 -0.66019   0.18991   0.28836  -0.2434\n",
      "  0.52784  -0.65762  -0.14081   1.0491    0.5134   -0.23816   0.69895\n",
      " -1.4813   -0.2487   -0.17936  -0.059137 -0.08056  -0.48782   0.014487\n",
      " -0.6259   -0.32367   0.41862  -1.0807    0.46742  -0.49931  -0.71895\n",
      "  0.86894   0.19539 ]\n",
      "in [ 0.085703 -0.22201   0.16569   0.13373   0.38239   0.35401   0.01287\n",
      "  0.22461  -0.43817   0.50164  -0.35874  -0.34983   0.055156  0.69648\n",
      " -0.17958   0.067926  0.39101   0.16039  -0.26635  -0.21138   0.53698\n",
      "  0.49379   0.9366    0.66902   0.21793  -0.46642   0.22383  -0.36204\n",
      " -0.17656   0.1748   -0.20367   0.13931   0.019832 -0.10413  -0.20244\n",
      "  0.55003  -0.1546    0.98655  -0.26863  -0.2909   -0.32866  -0.34188\n",
      " -0.16943  -0.42001  -0.046727 -0.16327   0.70824  -0.74911  -0.091559\n",
      " -0.96178  -0.19747   0.10282   0.55221   1.3816   -0.65636  -3.2502\n",
      " -0.31556  -1.2055    1.7709    0.4026   -0.79827   1.1597   -0.33042\n",
      "  0.31382   0.77386   0.22595   0.52471  -0.034053  0.32048   0.079948\n",
      "  0.17752  -0.49426  -0.70045  -0.44569   0.17244   0.20278   0.023292\n",
      " -0.20677  -1.0158    0.18325   0.56752   0.31821  -0.65011   0.68277\n",
      " -0.86585  -0.059392 -0.29264  -0.55668  -0.34705  -0.32895   0.40215\n",
      " -0.12746  -0.20228   0.87368  -0.545     0.79205  -0.20695  -0.074273\n",
      "  0.75808  -0.34243 ]\n",
      "Found 6518 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# dictionary of just words were using\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word in word_index.keys():\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "c = 0\n",
    "for word, word_vector in embeddings_index.items():\n",
    "    c += 1\n",
    "    if c <= 5:\n",
    "        print(word, word_vector)\n",
    "# print(embeddings_index)\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict of whole whole glove\n",
    "# embeddings_index = {}\n",
    "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.046539  ,  0.61966002,  0.56647003, -0.46584001, -1.18900001,\n",
       "         0.44599   ,  0.066035  ,  0.31909999,  0.14679   , -0.22119001,\n",
       "         0.79238999,  0.29905   ,  0.16073   ,  0.025324  ,  0.18678001,\n",
       "        -0.31000999, -0.28108001,  0.60514998, -1.0654    ,  0.52476001,\n",
       "         0.064152  ,  1.03579998, -0.40779001, -0.38011   ,  0.30801001,\n",
       "         0.59964001, -0.26991001, -0.76034999,  0.94221997, -0.46919   ,\n",
       "        -0.18278   ,  0.90652001,  0.79671001,  0.24824999,  0.25713   ,\n",
       "         0.6232    , -0.44768   ,  0.65357   ,  0.76902002, -0.51229   ,\n",
       "        -0.44332999, -0.21867   ,  0.38370001, -1.14830005, -0.94397998,\n",
       "        -0.15062   ,  0.30012   , -0.57805997,  0.20175   , -1.65910006,\n",
       "        -0.079195  ,  0.026423  ,  0.22051001,  0.99713999, -0.57538998,\n",
       "        -2.72659993,  0.31448001,  0.70521998,  1.43809998,  0.99125999,\n",
       "         0.13976   ,  1.34739995, -1.1753    ,  0.0039503 ,  1.02980006,\n",
       "         0.064637  ,  0.90886998,  0.82871997, -0.47003001, -0.10575   ,\n",
       "         0.5916    , -0.42210001,  0.57331002, -0.54114002,  0.10768   ,\n",
       "         0.39783999, -0.048744  ,  0.064596  , -0.61436999, -0.28600001,\n",
       "         0.50669998, -0.49757999, -0.81569999,  0.16407999, -1.96300006,\n",
       "        -0.26693001, -0.37593001, -0.95846999, -0.85839999, -0.71577001,\n",
       "        -0.32343   , -0.43121001,  0.41391999,  0.28374001, -0.70931   ,\n",
       "         0.15003   , -0.2154    , -0.37616   , -0.032502  ,  0.80620003]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model(model,xtr,ytr,xts,yts):\n",
    "    model.fit(xtr,ytr,batch_size=32,epochs=10,verbose=1,validation_split=0.3)\n",
    "    print(' ')\n",
    "    model.evaluate(xts,yts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "def get_LSTM_v4():    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False))\n",
    "#     model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2,activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc',metrics.binary_accuracy])\n",
    "\n",
    "    print('Train...')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 2730 samples, validate on 1170 samples\n",
      "Epoch 1/10\n",
      "2730/2730 [==============================] - 10s 4ms/step - loss: 0.3504 - acc: 0.8989 - binary_accuracy: 0.8989 - val_loss: 0.3203 - val_acc: 0.9231 - val_binary_accuracy: 0.9231\n",
      "Epoch 2/10\n",
      "2730/2730 [==============================] - 8s 3ms/step - loss: 0.2563 - acc: 0.9403 - binary_accuracy: 0.9403 - val_loss: 0.1757 - val_acc: 0.9444 - val_binary_accuracy: 0.9444\n",
      "Epoch 3/10\n",
      "2730/2730 [==============================] - 8s 3ms/step - loss: 0.2105 - acc: 0.9553 - binary_accuracy: 0.9553 - val_loss: 0.1473 - val_acc: 0.9530 - val_binary_accuracy: 0.9530\n",
      "Epoch 4/10\n",
      "2730/2730 [==============================] - 8s 3ms/step - loss: 0.1628 - acc: 0.9619 - binary_accuracy: 0.9619 - val_loss: 0.1451 - val_acc: 0.9470 - val_binary_accuracy: 0.9470\n",
      "Epoch 5/10\n",
      "2730/2730 [==============================] - 8s 3ms/step - loss: 0.1320 - acc: 0.9560 - binary_accuracy: 0.9560 - val_loss: 0.1319 - val_acc: 0.9530 - val_binary_accuracy: 0.9530\n",
      "Epoch 6/10\n",
      "2730/2730 [==============================] - 8s 3ms/step - loss: 0.1172 - acc: 0.9674 - binary_accuracy: 0.9674 - val_loss: 0.1584 - val_acc: 0.9547 - val_binary_accuracy: 0.9547\n",
      "Epoch 7/10\n",
      "2730/2730 [==============================] - 8s 3ms/step - loss: 0.1412 - acc: 0.9648 - binary_accuracy: 0.9648 - val_loss: 0.1071 - val_acc: 0.9615 - val_binary_accuracy: 0.9615\n",
      "Epoch 8/10\n",
      "2730/2730 [==============================] - 8s 3ms/step - loss: 0.0881 - acc: 0.9714 - binary_accuracy: 0.9714 - val_loss: 0.0968 - val_acc: 0.9675 - val_binary_accuracy: 0.9675\n",
      "Epoch 9/10\n",
      "2730/2730 [==============================] - 8s 3ms/step - loss: 0.0860 - acc: 0.9751 - binary_accuracy: 0.9751 - val_loss: 0.1367 - val_acc: 0.9615 - val_binary_accuracy: 0.9615\n",
      "Epoch 10/10\n",
      "2730/2730 [==============================] - 8s 3ms/step - loss: 0.0777 - acc: 0.9766 - binary_accuracy: 0.9766 - val_loss: 0.0990 - val_acc: 0.9632 - val_binary_accuracy: 0.9632\n",
      " \n",
      "1672/1672 [==============================] - 1s 475us/step\n"
     ]
    }
   ],
   "source": [
    "m = get_LSTM_v4()\n",
    "check_model(m,cnn_texts_mat,y_train,cnn_texts_mat_tst ,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8921"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
